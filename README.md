# ShiftBench

**A Benchmark Suite for Distribution Shift Evaluation**

ShiftBench provides a standardized benchmark for evaluating machine learning models under covariate shift, with a focus on reproducibility and honest abstention when estimates are unreliable.

## What ShiftBench Does

ShiftBench answers the question: *"How do different shift-aware evaluation methods compare across diverse datasets with documented covariate shifts?"*

**Key Features**:
- **50+ datasets** across molecular, text, and tabular domains with documented shifts
- **10+ baseline methods** spanning density ratio estimation, conformal prediction, and robust optimization
- **Reproducibility-first**: Hash-chained receipts make every result independently verifiable
- **Honest abstention**: Methods can declare NO-GUARANTEE when weights are unstable
- **Standardized protocol**: Certify-or-abstain framework with FWER control

## Quick Start

### Installation

```bash
# Basic installation
pip install -e .

# With development dependencies
pip install -e ".[dev]"

# With RAVEL support
pip install -e ".[ravel]"
```

### Load a Dataset

```python
from shiftbench.data import load_dataset, get_registry

# List available datasets
registry = get_registry()
print(registry.list_datasets(domain="molecular"))

# Load a dataset
X, y, cohorts, splits = load_dataset("bace")

# Split into calibration and target
cal_mask = (splits["split"] == "cal").values
test_mask = (splits["split"] == "test").values

X_cal, y_cal = X[cal_mask], y[cal_mask]
X_test, y_test = X[test_mask], y[test_mask]
```

### Evaluate a Method

```python
from shiftbench.baselines.ravel import create_ravel_baseline
from shiftbench.baselines.ulsif import create_ulsif_baseline

# Create method instances
ravel = create_ravel_baseline()
ulsif = create_ulsif_baseline()

# Estimate importance weights
weights_ravel = ravel.estimate_weights(X_cal, X_test)
weights_ulsif = ulsif.estimate_weights(X_cal, X_test)

# Get predictions (from your model)
predictions_cal = my_model.predict(X_cal)

# Estimate PPV bounds
tau_grid = [0.5, 0.6, 0.7, 0.8, 0.85, 0.9]
decisions_ravel = ravel.estimate_bounds(
    y_cal, predictions_cal, cohorts[cal_mask], weights_ravel, tau_grid
)
decisions_ulsif = ulsif.estimate_bounds(
    y_cal, predictions_cal, cohorts[cal_mask], weights_ulsif, tau_grid
)

# Compare results
for d in decisions_ravel:
    print(f"RAVEL: {d.cohort_id} @ Ï„={d.tau}: {d.decision} (lb={d.lower_bound:.3f})")
for d in decisions_ulsif:
    print(f"uLSIF: {d.cohort_id} @ Ï„={d.tau}: {d.decision} (lb={d.lower_bound:.3f})")
```

## Current Status (Phase 0)

### Implemented âœ…

**Infrastructure**:
- [x] Dataset registry (`data/registry.json`)
- [x] Baseline interface (`BaselineMethod` abstract class)
- [x] Dataset loader (`load_dataset()`)
- [x] RAVEL baseline wrapper
- [x] uLSIF baseline implementation

**Datasets** (11 molecular):
- [x] BACE, BBBP, ClinTox, ESOL, FreeSolv, Lipophilicity
- [x] SIDER, Tox21, ToxCast, MUV, MolHIV

### In Progress ðŸš§

**Baselines** (Priority 1):
- [ ] KLIEP (KL importance estimation)
- [ ] KMM (kernel mean matching)
- [ ] RULSIF (relative uLSIF)
- [ ] Weighted conformal prediction

**Datasets** (Expansion):
- [ ] Text datasets (AG News, IMDB, Civil Comments)
- [ ] Tabular datasets (Adult, OpenML)

**Infrastructure**:
- [ ] Evaluation harness (`python -m shiftbench.evaluate`)
- [ ] Results aggregation script
- [ ] Receipt generation for all methods

## Baseline Methods

### Tier 1: Density Ratio Estimation
1. **RAVEL** âœ… - Discriminative classifier + stability gates
2. **uLSIF** âœ… - Least-squares fitting (closed-form)
3. **KLIEP** ðŸš§ - KL minimization
4. **KMM** ðŸš§ - Kernel mean matching
5. **RULSIF** ðŸš§ - Relative density ratio

### Tier 2: Conformal Prediction
6. **Weighted Conformal** ðŸš§ - Tibshirani et al. 2019
7. **Split Conformal** ðŸš§ - Baseline (no shift adaptation)
8. **CV+** ðŸš§ - Cross-validation conformal

### Tier 3: Robust Optimization
9. **Group DRO** ðŸš§ - Sagawa et al. 2020
10. **Chi-Sq DRO** ðŸš§ - Duchi & Namkoong 2019

## Datasets

### Molecular (11/30 target)
All use scaffold-based splits to create covariate shift:
- **BACE** (1513 samples) - BACE inhibition
- **BBBP** (1975 samples) - Blood-brain barrier penetration
- **ClinTox** (1458 samples) - Clinical trial toxicity
- **ESOL** (1117 samples) - Aqueous solubility
- **FreeSolv** (642 samples) - Hydration free energy
- **Lipophilicity** (4200 samples) - Octanol/water distribution
- **SIDER** (1427 samples) - Side effects
- **Tox21** (7831 samples) - Nuclear receptor toxicity
- **ToxCast** (8576 samples) - High-throughput toxicity
- **MUV** (93087 samples) - Virtual screening
- **MolHIV** (41120 samples) - HIV inhibition

### Text (0/40 target)
ðŸš§ Coming soon

### Tabular (0/30 target)
ðŸš§ Coming soon

## Project Structure

```
shift-bench/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ registry.json          # Dataset metadata
â”‚   â””â”€â”€ processed/             # Preprocessed datasets
â”‚       â””â”€â”€ <dataset>/
â”‚           â”œâ”€â”€ features.npy
â”‚           â”œâ”€â”€ labels.npy
â”‚           â”œâ”€â”€ cohorts.npy
â”‚           â””â”€â”€ splits.csv
â”œâ”€â”€ src/shiftbench/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data.py                # Dataset loading
â”‚   â””â”€â”€ baselines/
â”‚       â”œâ”€â”€ base.py            # Abstract interface
â”‚       â”œâ”€â”€ ravel.py           # RAVEL implementation âœ…
â”‚       â””â”€â”€ ulsif.py           # uLSIF implementation âœ…
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ aggregate_results.py  # Results aggregation (TBD)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ SUBMISSION_GUIDE.md   # How to submit methods (TBD)
â”‚   â”œâ”€â”€ ADDING_DATASETS.md    # How to add datasets (TBD)
â”‚   â””â”€â”€ ADDING_METHODS.md     # How to implement baselines (TBD)
â””â”€â”€ README.md                  # This file
```

## Contributing

See [docs/ADDING_METHODS.md](docs/ADDING_METHODS.md) for how to implement a new baseline method.

See [docs/ADDING_DATASETS.md](docs/ADDING_DATASETS.md) for how to contribute datasets.

## Roadmap

**Phase 1** (Weeks 1-4): Foundation + External Baselines
- âœ… Dataset registry and loader
- âœ… Baseline interface
- âœ… RAVEL + uLSIF implementations
- ðŸš§ KLIEP, KMM, RULSIF implementations
- ðŸš§ Weighted conformal prediction

**Phase 2** (Weeks 5-8): Infrastructure + Full Benchmark
- ðŸš§ Evaluation harness
- ðŸš§ Full benchmark sweep (10 methods Ã— 50 datasets)
- ðŸš§ Results aggregation
- ðŸš§ Static leaderboard

**Phase 3** (Weeks 9-12): Documentation + Paper
- ðŸš§ Community documentation
- ðŸš§ NeurIPS D&B paper
- ðŸš§ Reproducibility artifacts

## Citation

```bibtex
@software{shiftbench2025,
  title = {ShiftBench: A Benchmark Suite for Distribution Shift Evaluation},
  author = {[Authors]},
  year = {2025},
  url = {https://github.com/anthropics/shift-bench}
}
```

## License

MIT

## Acknowledgments

- MoleculeNet for molecular datasets
- RAVEL project for baseline implementation
- All baseline method authors (see individual method papers)
