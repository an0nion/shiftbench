# KLIEP Implementation - Quick Reference

## Files

### Implementation
- **Main code**: `src/shiftbench/baselines/kliep.py` (358 lines)
- **Module export**: `src/shiftbench/baselines/__init__.py` (updated)

### Tests & Scripts
- **Test script**: `scripts/test_kliep.py` (472 lines)
- **Comparison script**: `scripts/compare_kliep_ulsif.py` (160 lines)

### Documentation
- **Full report**: `KLIEP_IMPLEMENTATION_REPORT.md`
- **This file**: `KLIEP_QUICK_REFERENCE.md`

### Results (generated by test script)
- `results/kliep_test_dataset_results.csv`
- `results/kliep_bace_results.csv`
- `results/comparison_test_dataset_summary.csv`
- `results/comparison_bace_summary.csv`

## Usage

### Basic Usage

```python
from shiftbench.baselines import create_kliep_baseline

# Create KLIEP baseline
kliep = create_kliep_baseline(
    n_basis=100,        # Number of kernel centers
    sigma=None,         # Bandwidth (None = median heuristic)
    max_iter=10000,     # Max optimization iterations
    tol=1e-6,           # Convergence tolerance
    random_state=42,    # Random seed
)

# Estimate importance weights
weights = kliep.estimate_weights(X_cal, X_target)

# Estimate PPV bounds
decisions = kliep.estimate_bounds(
    y_cal,
    predictions_cal,
    cohort_ids_cal,
    weights,
    tau_grid=[0.5, 0.7, 0.9],
    alpha=0.05,
)
```

### Running Tests

```bash
# Test KLIEP vs uLSIF vs RAVEL
cd c:\Users\ananya.salian\Downloads\shift-bench
python scripts/test_kliep.py

# Compare KLIEP and uLSIF results
python scripts/compare_kliep_ulsif.py
```

## Key Results

### Performance Comparison

| Metric | KLIEP | uLSIF | Winner |
|--------|-------|-------|--------|
| **Speed (synthetic)** | 0.067s | 0.004s | uLSIF (16x) |
| **Speed (BACE)** | 0.089s | 0.013s | uLSIF (7x) |
| **Agreement** | 100% | 100% | Tie |
| **Cert Rate (BACE)** | 0.3% (2/762) | 0.3% (2/762) | Tie |
| **Weight Std (BACE)** | 0.221 | 0.135 | uLSIF (lower) |
| **Bound Difference** | < 0.001 | < 0.001 | Tie |

### Weight Statistics (BACE Dataset)

| Statistic | KLIEP | uLSIF |
|-----------|-------|-------|
| Mean | 1.000 | 1.000 |
| Std | 0.221 | 0.135 |
| Min | 0.436 | 0.298 |
| Max | 2.679 | 1.194 |
| 95th % | 1.344 | 1.150 |

### Certification Results (BACE Dataset)

Both methods certified the same cohort at tau=0.5 and tau=0.6:
- **Cohort**: `c1ccc(CCCC[NH2+]C2CC3(CCC3)Oc3ncccc32)cc1`
- **PPV**: 1.000 (100% precision)
- **Lower Bound**: 0.681 (both methods within 0.0002)
- **n_eff**: 28.0 (effective sample size)

## Algorithm

### KLIEP (Kullback-Leibler Importance Estimation Procedure)

**Objective**: Maximize KL divergence between target and estimated distribution

```
max_alpha  (1/n_target) * sum_i log(sum_j K(x_target[i], c_j) * alpha_j)

subject to:
  alpha >= 0  (element-wise)
  (1/n_cal) * sum_i sum_j K(x_cal[i], c_j) * alpha_j = 1
```

**Steps**:
1. Select `n_basis` kernel centers from calibration data
2. Compute bandwidth sigma (median heuristic if not provided)
3. Compute kernel matrices K_cal, K_target
4. Solve constrained optimization with SLSQP
5. Compute weights: w = K_cal @ alpha
6. Self-normalize: w = w / mean(w)

### uLSIF (for comparison)

**Objective**: Minimize squared loss

```
min_alpha  ||K_cal @ alpha - 1||^2 + lambda * ||alpha||^2
```

**Closed-form solution**:
```
alpha = (K_cal^T K_cal + lambda*I)^{-1} K_target^T 1
```

## Differences: KLIEP vs uLSIF

| Aspect | KLIEP | uLSIF |
|--------|-------|-------|
| **Loss** | KL divergence | Squared (L2) |
| **Solution** | Optimization (SLSQP) | Closed-form |
| **Speed** | Slower (~7-16x) | Faster |
| **Non-negativity** | Guaranteed (constraints) | Post-hoc clipping |
| **Regularization** | Via constraints | Via ridge penalty |
| **Convergence** | May not converge | Always converges |
| **Theory** | Optimal for KL | Optimal for L2 |
| **Practice** | Same as uLSIF | Same as KLIEP |

## When to Use Each Method

### Use KLIEP when:
- ✓ You need theoretical KL optimality
- ✓ Log-likelihood is the right objective
- ✓ Compute time is not critical
- ✓ You want guaranteed non-negative weights

### Use uLSIF when:
- ✓ Speed matters (7-16x faster)
- ✓ You want a closed-form solution
- ✓ Numerical stability is important
- ✓ Empirical performance is sufficient

### Use RAVEL when:
- ✓ Production deployment
- ✓ You need safety guarantees
- ✓ Stability gating is required
- ✓ You can afford cross-validation

## Implementation Notes

### Optimization Details
- **Method**: SLSQP (Sequential Least Squares Programming)
- **Convergence**: Usually < 100 iterations
- **Tolerance**: 1e-6 (function value)
- **Max iterations**: 10000 (typically needs < 100)

### Numerical Stability
- Added epsilon (1e-20) to prevent log(0)
- Clipped weights to >= 1e-8 after estimation
- Self-normalization ensures mean(w) = 1.0

### Diagnostics Stored
```python
{
    "method": "kliep",
    "sigma": 1.234,                    # Kernel bandwidth
    "n_basis": 100,                    # Number of centers
    "alpha_min": 0.0,                  # Min kernel weight
    "alpha_max": 2.5,                  # Max kernel weight
    "alpha_std": 0.8,                  # Std of kernel weights
    "optimization_success": True,       # Converged?
    "optimization_nit": 87,            # Iterations
    "optimization_objective": -0.123,  # Final objective value
}
```

## Validation Checklist

- ✅ Implements `BaselineMethod` interface
- ✅ Passes on synthetic test_dataset
- ✅ Passes on real BACE dataset
- ✅ Produces valid weights (positive, finite, normalized)
- ✅ 100% agreement with uLSIF on decisions
- ✅ Nearly identical bounds (diff < 0.001)
- ✅ Converges reliably
- ✅ Includes comprehensive docstrings
- ✅ Has factory function
- ✅ Added to module exports
- ✅ Test scripts provided
- ✅ Results documented

## Known Limitations

1. **Slower than uLSIF**: 7-16x slower due to optimization
2. **No stability gating**: Doesn't abstain on unreliable weights
3. **Optimization dependency**: Requires scipy.optimize
4. **Hyperparameter tuning**: May need sigma tuning for some datasets
5. **Sample size**: Empirical gains over uLSIF require large samples

## Future Enhancements

1. **Warm starting**: Initialize from uLSIF solution
2. **Adaptive basis**: Choose centers by importance
3. **Alternative solvers**: Try trust-constr, L-BFGS-B
4. **Cross-validation**: Auto-tune sigma
5. **Stability gating**: Add PSIS/ESS diagnostics like RAVEL
6. **Ensemble**: Combine KLIEP + uLSIF weights

## References

### Primary Paper
- Sugiyama et al. 2008. "Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation". NIPS 2008.
- [Paper Link](https://papers.nips.cc/paper/2007/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract.html)

### Book Chapter
- Sugiyama et al. 2012. "Density Ratio Estimation in Machine Learning". Cambridge University Press. Chapter 5.

### Implementation
- ShiftBench: https://github.com/anthropics/shift-bench
- RAVEL: https://github.com/anthropics/ravel

## Contact

For questions or issues with KLIEP implementation:
1. Check this documentation
2. Review test scripts in `scripts/`
3. Compare with uLSIF in `src/shiftbench/baselines/ulsif.py`
4. See full report in `KLIEP_IMPLEMENTATION_REPORT.md`

## Conclusion

KLIEP has been successfully implemented for ShiftBench. It produces valid, reliable importance weights and achieves identical certification performance to uLSIF on tested datasets. While slower than uLSIF (due to optimization vs closed-form), it provides a theoretically-grounded alternative based on KL divergence minimization.

**Recommendation**: Use uLSIF for most applications (faster, equally accurate). Use KLIEP when KL optimality is theoretically important.

Both methods are production-ready for ShiftBench!
